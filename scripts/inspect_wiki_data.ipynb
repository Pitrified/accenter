{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open a wiki file and learn something\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download raw data from:\n",
    "https://kaikki.org/dictionary/French/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record structure\n",
    "\n",
    "### Word: `word`\n",
    "\n",
    "It's the word.\n",
    "\n",
    "### Part of speech: `pos`\n",
    "\n",
    "We can get a set of good pos to use.\n",
    "\n",
    "### Meaning of the word: `senses`\n",
    "\n",
    "Can be more than one (it's a list).\n",
    "\n",
    "* `glosses`: The meaning of the word, also a list.\n",
    "* `raw_glosses`: A more informative definition.\n",
    "* `examples`: Examples.\n",
    "\n",
    "Each `sense` has a `categories` list,\n",
    "that can be useful for clustering words\n",
    "or to train by topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent words\n",
    "\n",
    "1. Get the big corpus.\n",
    "1. For each word, get the non inflected version.\n",
    "1. Compute freq lol you are done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from itertools import pairwise\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from random import randint\n",
    "\n",
    "from loguru import logger as lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg.remove()\n",
    "lg.add(sys.stderr, format=\"{message}\", level=\"DEBUG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fol = Path(\".\").absolute().parent / \"dataset\"\n",
    "\n",
    "wiki_fn_word_forms = \"kaikki.org-dictionary-French-words.json\"\n",
    "wiki_fn_non_inflected_senses = \"kaikki.org-dictionary-French-all-no-wFNY2q.json\"\n",
    "\n",
    "# wiki_fp = dataset_fol / wiki_fn_word_forms\n",
    "wiki_fp = dataset_fol / wiki_fn_non_inflected_senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accent = set(\"àÀâÂéÉèÈêÊëËîÎïÏœŒôÔùÙûÛüÜçÇ\")\n",
    "print(accent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pos = {\n",
    "    # \"\",\n",
    "    # '<i class=\"Jpan mention\" lang=\"ja\">かみかぜ</i> (kamikaze, “suicide flyer”, literally “divine wind”)',\n",
    "    # 'Modern French <i class=\"Latn mention\" lang=\"fr\">chair</i>',\n",
    "    # \"a\",\n",
    "    # \"a commune in Normandy, France\",\n",
    "    # \"a restoration of the Latin 3rd-person-singular -t\",\n",
    "    # \"abbrev\",\n",
    "    # \"ablative\",\n",
    "    # \"accusative plural\",\n",
    "    \"adj\",\n",
    "    # \"adjectival suffix\",\n",
    "    \"adjective\",\n",
    "    # \"adjective-forming suffix\",\n",
    "    \"adv\",\n",
    "    \"adverb\",\n",
    "    # \"affirmative particle\",\n",
    "    # \"affix\",\n",
    "    # \"an anchovy-based condiment\",\n",
    "    # \"an apocopic form of la, la before a vowel\",\n",
    "    \"article\",\n",
    "    # \"augmentative suffix\",\n",
    "    # \"character\",\n",
    "    # \"conj\",\n",
    "    # \"det\",\n",
    "    # \"dialectal\",\n",
    "    # \"diminutive ending\",\n",
    "    # \"diminutive suffix\",\n",
    "    # \"first-person plural present indicative ending\",\n",
    "    # 'from an Illyrian word probably from Proto-Indo-European <i class=\"Latinx mention\" lang=\"ine-pro\">*sab-</i> (“taste”)',\n",
    "    # \"infix\",\n",
    "    # \"instrumental suffix\",\n",
    "    # \"intensifier\",\n",
    "    # \"interfix\",\n",
    "    # \"interjection used in deer-hunting\",\n",
    "    # \"intj\",\n",
    "    # \"n\",\n",
    "    \"name\",\n",
    "    # \"name of a Celtic tribe in Southern Germany, which later emigrated to Gaul\",\n",
    "    # \"nominal suffix\",\n",
    "    \"noun\",\n",
    "    # \"noun suffix\",\n",
    "    \"nouns\",\n",
    "    \"num\",\n",
    "    # \"onomatopoeia of the lowing of cattle\",\n",
    "    # \"particle\",\n",
    "    # \"past participle of dire (“to say”)\",\n",
    "    # \"past passive participle\",\n",
    "    \"phrase\",\n",
    "    # \"postp\",\n",
    "    # \"prefix\",\n",
    "    # \"prep\",\n",
    "    # \"prep_phrase\",\n",
    "    \"pron\",\n",
    "    # \"pronounced /le‿ʁital(jɛ̃)/\",\n",
    "    \"proverb\",\n",
    "    # \"punct\",\n",
    "    # \"reflexive pronoun\",\n",
    "    # \"second-person singular\",\n",
    "    # \"stem libr-\",\n",
    "    # \"suffix\",\n",
    "    # \"suffix added to noun stems to form adjectives\",\n",
    "    # \"suffix added to verbal stems forming neuter nouns denoting the result of, a particular instance of, or the object of an action\",\n",
    "    # \"suffix denoting occupation\",\n",
    "    # \"suffix forming adjectives from nouns\",\n",
    "    # \"suffix forming adjectives meaning ‘belonging to, relating to’\",\n",
    "    # \"suffix forming augmentatives\",\n",
    "    # \"suffix forming diminutives\",\n",
    "    # \"suffix forming infinitives of first-conjugation verbs\",\n",
    "    # \"suffix forming nouns usually denoting diseased conditions\",\n",
    "    # \"suffix meaning ‘of or pertaining to’\",\n",
    "    # \"suffix with the sense ‘relating’ to forming adjectives\",\n",
    "    # \"surname\",\n",
    "    # \"symbol\",\n",
    "    # \"v\",\n",
    "    \"verb\",\n",
    "    \"verb and noun\",\n",
    "}\n",
    "len(good_pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_object(obj, level, seen_keys, seen_pos):\n",
    "    # pad = \"\\t\" * level + str(level) + \": \"\n",
    "    if isinstance(obj, dict):\n",
    "        for key in obj:\n",
    "            # print(f\"{pad}opening {key=} {obj[key]=}\")\n",
    "            seen_keys.add(key)\n",
    "            if key == \"pos\":\n",
    "                seen_pos.add(obj[key])\n",
    "            walk_object(obj[key], level + 1, seen_keys, seen_pos)\n",
    "\n",
    "    elif isinstance(obj, list):\n",
    "        for el in obj:\n",
    "            # print(f\"{pad}traversing {el}\")\n",
    "            walk_object(el, level + 1, seen_keys, seen_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_structure(obj, level, struct, curr_tag):\n",
    "    # build a pad to indent the levels\n",
    "    pad = f\"{'  '*level} {level}:\"\n",
    "    obj_str = f\"{obj}\"\n",
    "\n",
    "    # if it is a dict\n",
    "    if isinstance(obj, dict):\n",
    "        # print(f\"{pad} dict {obj_str[:30]}\")\n",
    "\n",
    "        # mark it as an object\n",
    "        curr_tag += \"#\"\n",
    "        struct[curr_tag] = 0\n",
    "        # print(f\"{pad}      {curr_tag}\")\n",
    "\n",
    "        # iterate all the keys\n",
    "        for key in obj:\n",
    "            # update the tag for this key\n",
    "            # print(f\"{pad} key {key}\")\n",
    "            new_tag = curr_tag + f\"{key}\"\n",
    "            # print(f\"{pad}      {new_tag}\")\n",
    "            struct[new_tag] = 0\n",
    "            if \"#\" in key or \">\" in key:\n",
    "                lg.warning(f\"Separator character in key {key}\")\n",
    "                # TODO escape the separator? by doubling it?\n",
    "\n",
    "            # recurse\n",
    "            build_structure(obj[key], level + 1, struct, new_tag)\n",
    "\n",
    "    # if it is a list\n",
    "    elif isinstance(obj, list):\n",
    "        # print(f\"{pad} list {obj_str[:30]}\")\n",
    "\n",
    "        # mark that there is a list here\n",
    "        curr_tag += \">\"\n",
    "        struct[curr_tag] = 0\n",
    "        # print(f\"{pad}      {curr_tag}\")\n",
    "\n",
    "        # check if its all made of strings, int, ... TODO\n",
    "        listlen = len(obj)\n",
    "        if listlen > 0 and all(isinstance(el, str) for el in obj):\n",
    "            curr_tag += \"str\"\n",
    "            struct[curr_tag] = 0\n",
    "        elif listlen > 0 and all(isinstance(el, int) for el in obj):\n",
    "            curr_tag += \"int\"\n",
    "            struct[curr_tag] = 0\n",
    "        # it is not a base type\n",
    "        else:\n",
    "            # iterate all objects: # we assume all the objects are homogeneous,\n",
    "            # at most they can have some missing keys\n",
    "            for i, el in enumerate(obj):\n",
    "                # print(f\"{pad} item {i}\")\n",
    "                build_structure(el, level + 1, struct, curr_tag)\n",
    "\n",
    "    # else: print(f\"{pad} final {obj_str[:30]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the struct into a list of strings\n",
    "\n",
    "\n",
    "def structure_to_strlist(struct):\n",
    "\n",
    "    struct_str_all: list[str] = sorted(struct.keys())\n",
    "\n",
    "    # if you are the prefix of something\n",
    "    # you are the prefix of the next in the sorted list\n",
    "    struct_str = []\n",
    "    for first, second in pairwise(struct_str_all):\n",
    "        if second.startswith(first):\n",
    "            continue\n",
    "        struct_str.append(first)\n",
    "    struct_str.append(struct_str_all[-1])\n",
    "\n",
    "    return struct_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_str_to_path(string):\n",
    "    return [x for x in re.split(\"[#>]\", string) if x]\n",
    "\n",
    "\n",
    "def match_str_to_path(string, path) -> tuple[bool, list]:\n",
    "    search_path = split_str_to_path(string)\n",
    "    if all(p == s for p, s in zip(path, search_path)):\n",
    "        matches = True\n",
    "        rest = search_path[len(path) :]\n",
    "    else:\n",
    "        matches = False\n",
    "        rest = []\n",
    "    return matches, rest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_keys(dict_, keys, which_filter):\n",
    "    orig_keys = list(dict_.keys())\n",
    "    for key in orig_keys:\n",
    "        # keep only the keys in keys\n",
    "        if which_filter == \"keep\":\n",
    "            if key not in keys:\n",
    "                del dict_[key]\n",
    "        # remove only the keys in keys\n",
    "        elif which_filter == \"remove\":\n",
    "            if key in keys:\n",
    "                del dict_[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the requested path, if it exists\n",
    "def access_by_path(obj, path):\n",
    "    # lg.debug(f\"{path} {obj}\")\n",
    "    # pprint((path, obj), width=100)\n",
    "\n",
    "    # if we have no more path, we have found the value!\n",
    "    if len(path) == 0:\n",
    "        return obj\n",
    "\n",
    "    key = path[0]\n",
    "    remaining_path = path[1:]\n",
    "\n",
    "    # if it is a dict search for the rest of the path in the value\n",
    "    if isinstance(obj, dict):\n",
    "        if key not in obj:\n",
    "            # None is actually a valid return value\n",
    "            # shoud raise KeyError\n",
    "            # but it's tricky, because if the first element in a list\n",
    "            # does not contain the key, it might be in the next one\n",
    "            # so in the list portion we should catch them while we still have elements\n",
    "            # and only raise it in the end\n",
    "            return None\n",
    "        return access_by_path(obj[key], remaining_path)\n",
    "\n",
    "    elif isinstance(obj, list):\n",
    "        # search for the rest of the path in all the objects in the list\n",
    "        for el in obj:\n",
    "            # search for this path continuation in this element\n",
    "            if key not in el:\n",
    "                continue\n",
    "            maybe_value = access_by_path(el[key], remaining_path)\n",
    "            # if we find something in this element, return that\n",
    "            if maybe_value is not None:\n",
    "                return maybe_value\n",
    "        # if all the elements failed return None\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One record per line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data_orig = []\n",
    "with wiki_fp.open() as wf:\n",
    "    for line in wf:\n",
    "        word_data = json.loads(line)\n",
    "        words_data_orig.append(word_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the struct of the original records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the thing on the whole dataset\n",
    "struct_all = {}\n",
    "for word_data in words_data_orig[:1000000]:\n",
    "    build_structure(word_data, 0, struct_all, \"\")\n",
    "\n",
    "struct_all_str = structure_to_strlist(struct_all)\n",
    "pprint(struct_all_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only interesting keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the word records\n",
    "words_data_all = deepcopy(words_data_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_keep_keys = [\n",
    "    \"categories\",\n",
    "    \"form_of\",\n",
    "    \"pos\",\n",
    "    \"senses\",\n",
    "    \"word\",\n",
    "]\n",
    "\n",
    "cat_keep_keys = [\"name\"]\n",
    "\n",
    "examples_keep_keys = [\"english\", \"text\"]\n",
    "\n",
    "sense_keep_keys = [\n",
    "    \"categories\",\n",
    "    \"examples\",\n",
    "    \"glosses\",\n",
    "    \"raw_glosses\",\n",
    "    \"tags\",\n",
    "    \"topics\",\n",
    "]\n",
    "\n",
    "# keep only good pos here\n",
    "words_data = []\n",
    "\n",
    "for word_data in words_data_all:\n",
    "\n",
    "    # only keep words with good pos\n",
    "    if word_data[\"pos\"] not in good_pos:\n",
    "        continue\n",
    "\n",
    "    # # keep only some keys in the word record\n",
    "    # word_data_keys = list(word_data.keys())\n",
    "    # for word_data_key in word_data_keys:\n",
    "    #     if word_data_key not in word_keep_keys:\n",
    "    #         del word_data[word_data_key]\n",
    "    filter_keys(word_data, word_keep_keys, \"keep\")\n",
    "\n",
    "    if \"categories\" in word_data:\n",
    "        for cat in word_data[\"categories\"]:\n",
    "            filter_keys(cat, cat_keep_keys, \"keep\")\n",
    "\n",
    "    for sense in word_data[\"senses\"]:\n",
    "\n",
    "        # remove unwanted sense keys\n",
    "        filter_keys(sense, sense_keep_keys, \"keep\")\n",
    "\n",
    "        # remove redundant info in categories\n",
    "        if \"categories\" in sense:\n",
    "            # iterate over all the categories\n",
    "            for cat in sense[\"categories\"]:\n",
    "\n",
    "                # # copy the list of keys to avoid changing len while looping\n",
    "                # cat_keys = list(cat.keys())\n",
    "                # for cat_key in cat_keys:\n",
    "                #     # delete all the unwanted keys from the cat object\n",
    "                #     if cat_key not in cat_keep_keys:\n",
    "                #         del cat[cat_key]\n",
    "                filter_keys(cat, cat_keep_keys, \"keep\")\n",
    "\n",
    "        if \"examples\" in sense:\n",
    "            for example in sense[\"examples\"]:\n",
    "                filter_keys(example, examples_keep_keys, \"keep\")\n",
    "\n",
    "    words_data.append(word_data)\n",
    "\n",
    "# sort(ish) the words\n",
    "# one word can have more than one pos\n",
    "words_data_sort = sorted(words_data, key=lambda x: x[\"word\"])\n",
    "\n",
    "print(f\"{len(words_data_all)=} {len(words_data_sort)=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save filtered data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # output file\n",
    "# wiki_out_fn = \"kaikki.org-dictionary-French-all-no-filter.json\"\n",
    "# wiki_out_fp = dataset_fol / wiki_out_fn\n",
    "# print(f\"{wiki_out_fp}\")\n",
    "# # build all the records\n",
    "# out_str = []\n",
    "# for word_data in words_data_sort:\n",
    "#     word_str = json.dumps(word_data)\n",
    "#     out_str.append(word_str)\n",
    "# # write out the records\n",
    "# dump_str = \"\\n\".join(out_str)\n",
    "# wiki_out_fp.write_text(dump_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect keys and pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_keys = set()\n",
    "seen_pos = set()\n",
    "\n",
    "for word_data in words_data:\n",
    "    walk_object(word_data, level=0, seen_keys=seen_keys, seen_pos=seen_pos)\n",
    "\n",
    "len(seen_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze some words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_acc_data = []\n",
    "word_search = []\n",
    "\n",
    "for word_data in words_data_sort:\n",
    "    word = word_data[\"word\"]\n",
    "\n",
    "    word_letters = set(word)\n",
    "    is_accent = accent.intersection(word_letters)\n",
    "    if is_accent:\n",
    "        words_acc_data.append(word_data)\n",
    "        # print(f\"accent! {word}\")\n",
    "        # break\n",
    "\n",
    "    # word with a space\n",
    "    # if \" \" in word:\n",
    "    #     print(f\"found! {word}\")\n",
    "    #     break\n",
    "\n",
    "    the_word = \"chaise\"\n",
    "    # the_word = \"abîme\"\n",
    "    # the_word = \"abime\"\n",
    "    # the_word = \"abimes\"\n",
    "    # the_word = \"angariés\"\n",
    "    # the_word = \"angaries\"\n",
    "    # the_word = \"arrière\"\n",
    "    # the_word = \"Alexia\"\n",
    "    if word == the_word:\n",
    "        print(f\"found! {word} {word_data['pos']}\")\n",
    "        word_search.append(word_data)\n",
    "\n",
    "    # if word_data[\"pos\"] == \"name\":\n",
    "    #     print(f\"found! {word} {word_data['pos']}\")\n",
    "    #     break\n",
    "\n",
    "    if word_data[\"pos\"] not in good_pos:\n",
    "        print(f\"yo why no filter me\")\n",
    "\n",
    "\n",
    "print(f\"{len(words_acc_data)=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(word_search[0], width=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ri = randint(0, len(words_data_sort) - 1)\n",
    "# word_data = words_data_sort[ri]\n",
    "# print(f\"{ri=} {word_data['word']}\")\n",
    "# pprint(word_data, width=150)\n",
    "\n",
    "ri = randint(0, len(words_acc_data) - 1)\n",
    "acc_data = words_acc_data[ri]\n",
    "print(f\"{ri=} {acc_data['word']}\")\n",
    "pprint(acc_data, width=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access_by_path(acc_data, [\"senses\", \"examples\"])\n",
    "# access_by_path(acc_data, [\"senses\", \"examples\", \"english\"])\n",
    "access_by_path(acc_data, [\"senses\", \"miss\", \"english\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the structure of the json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the thing on the whole dataset\n",
    "struct = {}\n",
    "for word_data in words_data_sort[:]:\n",
    "    build_structure(word_data, 0, struct, \"\")\n",
    "struct_str = structure_to_strlist(struct)\n",
    "pprint(struct_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search a sample of a path\n",
    "for word_data in words_data_sort[:]:\n",
    "    # data = access_by_path(word_data, [\"senses\", \"examples\", \"ref\"])\n",
    "    # data = access_by_path(word_data, [\"senses\", \"examples\", \"note\"])\n",
    "    data = access_by_path(word_data, [\"senses\", \"examples\", \"type\"])\n",
    "    if data is not None:\n",
    "        pprint(data)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search all sample in a path\n",
    "data_all = set()\n",
    "for word_data in words_data_sort[:]:\n",
    "    data = access_by_path(word_data, [\"senses\", \"examples\", \"type\"])\n",
    "    if data is not None:\n",
    "        data_all.add(data)\n",
    "\n",
    "pprint(data_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the structure of the json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a list of nodes? string of paths?\n",
    "# get the list of keys for a node\n",
    "\n",
    "import re\n",
    "\n",
    "rs_sep0 = \"[#>]{,2}\"\n",
    "rs_sep1 = \"[#>]{1,2}\"\n",
    "\n",
    "\n",
    "# path = []\n",
    "# search_in = \"#senses>#coordinate_terms>#english\"\n",
    "\n",
    "path = [\"senses\", \"coordinate_terms\"]\n",
    "search_in = \"#senses>#coordinate_terms>#english\"\n",
    "\n",
    "# path = [\"pos\"]\n",
    "# search_in = \"#pos\"\n",
    "\n",
    "# build a regex to match this path\n",
    "# we join in the center with the separator\n",
    "rs_path_part = rs_sep0.join([f\"{key}\" for key in path])\n",
    "# we add a separator in the beginning:\n",
    "# an empty path will still be populated\n",
    "rs_path_full = rs_sep0 + rs_path_part + rs_sep0 + \"(.*)\"\n",
    "print(f\"{rs_path_full=}\")\n",
    "re_path = re.compile(rs_path_full)\n",
    "print(f\"{re_path=}\")\n",
    "\n",
    "# parse the current path\n",
    "print(f\"{search_in=}\")\n",
    "match_full_key = re_path.match(search_in)\n",
    "print(f\"{match_full_key=}\")\n",
    "full_key_str = match_full_key.group(1)\n",
    "print(f\"{full_key_str=}\")\n",
    "\n",
    "# extract the key\n",
    "\n",
    "# first look for a separator after a key\n",
    "rs_key = \"(.*?)\" + rs_sep1\n",
    "re_key = re.compile(rs_key)\n",
    "print(f\"{re_key=}\")\n",
    "match_key = re_key.match(full_key_str)\n",
    "print(f\"{match_key=}\")\n",
    "\n",
    "# if we did not match then the whole str is the key\n",
    "if match_key is None:\n",
    "    print(f\"matched all {full_key_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_in = \"#senses>#coordinate_terms>#english\"\n",
    "print(f\"{search_in=}\")\n",
    "\n",
    "# rs_sep1_key = rs_sep1 + \"(.*?)\"\n",
    "# rs_sep1_key = rs_sep1 + \"(.*)\"\n",
    "rs_sep1_key = rs_sep1 + \"([a-z_]+)\"\n",
    "rs_sep1_key_rep = f\"(?:{rs_sep1_key})*\"\n",
    "print(f\"{rs_sep1_key_rep=}\")\n",
    "re_sep1_key_rep = re.compile(rs_sep1_key_rep)\n",
    "print(f\"{re_sep1_key_rep=}\")\n",
    "m_sep1_key_rep = re_sep1_key_rep.match(search_in)\n",
    "print(f\"{m_sep1_key_rep=}\")\n",
    "for g in m_sep1_key_rep.groups():\n",
    "    print(f\"{g=}\")\n",
    "\n",
    "# rs_boring_two = \"[#>]{1,2}(.*?)[#>]{1,2}(.*?)\"\n",
    "rs_boring_two = \"[#>]{1,2}(.*)[#>]{1,2}(.*)\"\n",
    "re_boring_two = re.compile(rs_boring_two)\n",
    "re_boring_two.match(search_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had fun with regex and groups but dear Lord, you can do this in 3 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = [\"senses\", \"coordinate_terms\", \"english\", \"other\"]\n",
    "# path = [\"senses\", \"coordinate_terms\", \"english\"]\n",
    "# path = [\"senses\", \"coordinate_terms\"]\n",
    "# path = [\"senses\"]\n",
    "path = [\"other\"]\n",
    "# path = []\n",
    "search_in = \"#senses>#coordinate_terms>#english\"\n",
    "\n",
    "search_path = [x for x in re.split(\"[#>]\", search_in) if x]\n",
    "if all(p == s for p, s in zip(path, search_path)):\n",
    "    rest = search_path[len(path) :]\n",
    "    print(f\"matches {rest}\")\n",
    "    if len(rest) == 0:\n",
    "        print(f\"matches but not longer than path\")\n",
    "\n",
    "match_str_to_path(search_in, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the wrong thing: objects should be homogeneous\n",
    "# at most you can miss a key in an object, that's ok\n",
    "# but all the objects in the list are the same thing\n",
    "nested_lists = [\n",
    "    {\"a\": \"data\"},\n",
    "    [\n",
    "        {\"a\": \"data\"},\n",
    "        {\"b\": \"data\"},\n",
    "    ],\n",
    "    [\n",
    "        [\n",
    "            {\"c\": \"data\"},\n",
    "            {\"d\": \"data\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"e\": \"data\"},\n",
    "            {\"f\": \"data\"},\n",
    "        ],\n",
    "    ],\n",
    "]\n",
    "struct_nest = {}\n",
    "build_structure(nested_lists, 0, struct_nest, \"\")\n",
    "for k in structure_to_strlist(struct_nest):\n",
    "    # for k in struct_nest.keys():\n",
    "    print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an object can have a list as a *value*, that's ok\n",
    "nested_lists = [\n",
    "    {\n",
    "        \"a\": \"data\",\n",
    "        \"b\": {\"b0\": \"data\"},\n",
    "        \"c\": [\n",
    "            {\"c0\": \"data0\", \"c1\": \"data0\"},\n",
    "            {\"c0\": \"data1\"},\n",
    "        ],\n",
    "        \"d\": [\n",
    "            [\n",
    "                {\"d0\": \"data\", \"d1\": \"data\"},\n",
    "            ],\n",
    "            [\n",
    "                {\"d1\": \"data\", \"d1\": \"data\"},\n",
    "            ],\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "struct_nest = {}\n",
    "build_structure(nested_lists, 0, struct_nest, \"\")\n",
    "for k in structure_to_strlist(struct_nest):\n",
    "    # for k in struct_nest.keys():\n",
    "    print(k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56fddae659bb42062d07dbb89391b2c426532002bd0fbd34249c182257405271"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
